{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "# Deep Deterministic Policy Gradient (DDPG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## Introduction\n",
    "\n",
    "**Deep Deterministic Policy Gradient (DDPG)** is a model-free off-policy algorithm designed for learning continuous actions. It amalgamates concepts from Deterministic Policy Gradient (DPG) and Deep Q-Network (DQN) methodologies. Incorporating Experience Replay and slow-learning target networks from DQN, DDPG operates effectively in continuous action spaces, making it suitable for various real-world applications.\n",
    "\n",
    "This tutorial closely follows the seminal paper titled [Continuous Control with Deep Reinforcement Learning](https://arxiv.org/abs/1509.02971) by Lillicrap et al.\n",
    "\n",
    "## Problem Statement\n",
    "\n",
    "We aim to tackle the classical **Inverted Pendulum** control problem, wherein the agent must decide between two actions: swinging left or swinging right. However, unlike discrete action spaces where choices are limited to a predefined set (e.g., -1 or +1), this problem presents continuous actions, ranging infinitely between -2 and +2. This continuous nature poses a challenge for traditional Q-Learning Algorithms.\n",
    "\n",
    "## Quick Theory Overview\n",
    "\n",
    "DDPG employs two key neural networks:\n",
    "\n",
    "1. **Actor:** This network proposes an action based on the current state.\n",
    "2. **Critic:** Evaluates the goodness of an action given a state and an action, predicting positive values for good actions and negative values for bad actions.\n",
    "\n",
    "In addition to these networks, DDPG incorporates two crucial techniques not found in the original DQN:\n",
    "\n",
    "**Firstly, it employs two Target Networks.**\n",
    "\n",
    "**Why?** These networks enhance training stability. Essentially, by learning from estimated targets and updating the Target Networks slowly, the algorithm maintains stable estimated targets. Conceptually, this approach can be likened to saying, \"I have a decent strategy; let me stick with it for a while until I find a better one,\" rather than starting afresh after every action. Refer to this [StackOverflow answer](https://stackoverflow.com/a/54238556/13475679) for further insights.\n",
    "\n",
    "**Secondly, it utilizes Experience Replay.**\n",
    "\n",
    "Here, a list of tuples `(state, action, reward, next_state)` is maintained. Instead of solely learning from recent experiences, the algorithm learns from a sample of all accumulated experiences. This aids in improving sample efficiency and stabilizing training.\n",
    "\n",
    "Now, let's delve into the implementation details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-27 12:44:40.475644: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-03-27 12:44:41.268863: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "#Imports.\n",
    "import os\n",
    "os.environ[\"KERAS_BACKEND\"] = \"tensorflow\"\n",
    "import keras\n",
    "from keras import layers\n",
    "import tensorflow as tf\n",
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "We are using [Gymnasium](https://gymnasium.farama.org/) to create the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of State Space ->  3\n",
      "Size of Action Space ->  1\n",
      "Max Value of Action ->  2.0\n",
      "Min Value of Action ->  -2.0\n"
     ]
    }
   ],
   "source": [
    "# Specify the render_mode parameter to show the attempts of the agent in a pop-up window\n",
    "env = gym.make(\"Pendulum-v1\", render_mode=\"human\")\n",
    "\n",
    "num_states = env.observation_space.shape[0]\n",
    "print(\"Size of State Space ->  {}\".format(num_states))\n",
    "num_actions = env.action_space.shape[0]\n",
    "print(\"Size of Action Space ->  {}\".format(num_actions))\n",
    "\n",
    "upper_bound = env.action_space.high[0]\n",
    "lower_bound = env.action_space.low[0]\n",
    "\n",
    "print(\"Max Value of Action ->  {}\".format(upper_bound))\n",
    "print(\"Min Value of Action ->  {}\".format(lower_bound))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "To implement better exploration using noisy perturbations, particularly an Ornstein-Uhlenbeck process for generating noise, we can utilize a Python class to encapsulate this functionality. Below is a simple implementation of the Ornstein-Uhlenbeck process for generating noise:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "class OUActionNoise:\n",
    "    def __init__(self, mean, std_deviation, theta=0.15, dt=1e-2, x_initial=None):\n",
    "        self.theta = theta\n",
    "        self.mean = mean\n",
    "        self.std_dev = std_deviation\n",
    "        self.dt = dt\n",
    "        self.x_initial = x_initial\n",
    "        self.reset()\n",
    "\n",
    "    def __call__(self):\n",
    "        # Formula taken from https://www.wikipedia.org/wiki/Ornstein-Uhlenbeck_process\n",
    "        x = (\n",
    "            self.x_prev\n",
    "            + self.theta * (self.mean - self.x_prev) * self.dt\n",
    "            + self.std_dev * np.sqrt(self.dt) * np.random.normal(size=self.mean.shape)\n",
    "        )\n",
    "        # Store x into x_prev\n",
    "        # Makes next noise dependent on current one\n",
    "        self.x_prev = x\n",
    "        return x\n",
    "\n",
    "    def reset(self):\n",
    "        if self.x_initial is not None:\n",
    "            self.x_prev = self.x_initial\n",
    "        else:\n",
    "            self.x_prev = np.zeros_like(self.mean)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "The `Buffer` class implements Experience Replay.\n",
    "\n",
    "---\n",
    "![Algorithm](https://i.imgur.com/mS6iGyJ.jpg)\n",
    "---\n",
    "\n",
    "\n",
    "**Critic loss** - Mean Squared Error of `y - Q(s, a)`\n",
    "where `y` is the expected return as seen by the Target network,\n",
    "and `Q(s, a)` is action value predicted by the Critic network. `y` is a moving target\n",
    "that the critic model tries to achieve; we make this target\n",
    "stable by updating the Target model slowly.\n",
    "\n",
    "**Actor loss** - This is computed using the mean of the value given by the Critic network\n",
    "for the actions taken by the Actor network. We seek to maximize this quantity.\n",
    "\n",
    "Hence we update the Actor network so that it produces actions that get\n",
    "the maximum predicted value as seen by the Critic, for a given state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "class Buffer:\n",
    "    def __init__(self, buffer_capacity=100000, batch_size=64):\n",
    "        # Number of \"experiences\" to store at max\n",
    "        self.buffer_capacity = buffer_capacity\n",
    "        # Num of tuples to train on.\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "        # Its tells us num of times record() was called.\n",
    "        self.buffer_counter = 0\n",
    "\n",
    "        # Instead of list of tuples as the exp.replay concept go\n",
    "        # We use different np.arrays for each tuple element\n",
    "        self.state_buffer = np.zeros((self.buffer_capacity, num_states))\n",
    "        self.action_buffer = np.zeros((self.buffer_capacity, num_actions))\n",
    "        self.reward_buffer = np.zeros((self.buffer_capacity, 1))\n",
    "        self.next_state_buffer = np.zeros((self.buffer_capacity, num_states))\n",
    "\n",
    "    # Takes (s,a,r,s') obervation tuple as input\n",
    "    def record(self, obs_tuple):\n",
    "        # Set index to zero if buffer_capacity is exceeded,\n",
    "        # replacing old records\n",
    "        index = self.buffer_counter % self.buffer_capacity\n",
    "\n",
    "        self.state_buffer[index] = obs_tuple[0]\n",
    "        self.action_buffer[index] = obs_tuple[1]\n",
    "        self.reward_buffer[index] = obs_tuple[2]\n",
    "        self.next_state_buffer[index] = obs_tuple[3]\n",
    "\n",
    "        self.buffer_counter += 1\n",
    "\n",
    "    # Eager execution is turned on by default in TensorFlow 2. Decorating with tf.function allows\n",
    "    # TensorFlow to build a static graph out of the logic and computations in our function.\n",
    "    # This provides a large speed up for blocks of code that contain many small TensorFlow operations such as this one.\n",
    "    @tf.function\n",
    "    def update(\n",
    "        self,\n",
    "        state_batch,\n",
    "        action_batch,\n",
    "        reward_batch,\n",
    "        next_state_batch,\n",
    "    ):\n",
    "        # Training and updating Actor & Critic networks.\n",
    "        # See Pseudo Code.\n",
    "        with tf.GradientTape() as tape:\n",
    "            target_actions = target_actor(next_state_batch, training=True)\n",
    "            y = reward_batch + gamma * target_critic(\n",
    "                [next_state_batch, target_actions], training=True\n",
    "            )\n",
    "            critic_value = critic_model([state_batch, action_batch], training=True)\n",
    "            critic_loss = keras.ops.mean(keras.ops.square(y - critic_value))\n",
    "\n",
    "        critic_grad = tape.gradient(critic_loss, critic_model.trainable_variables)\n",
    "        critic_optimizer.apply_gradients(\n",
    "            zip(critic_grad, critic_model.trainable_variables)\n",
    "        )\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "            actions = actor_model(state_batch, training=True)\n",
    "            critic_value = critic_model([state_batch, actions], training=True)\n",
    "            # Used `-value` as we want to maximize the value given\n",
    "            # by the critic for our actions\n",
    "            actor_loss = -keras.ops.mean(critic_value)\n",
    "\n",
    "        actor_grad = tape.gradient(actor_loss, actor_model.trainable_variables)\n",
    "        actor_optimizer.apply_gradients(\n",
    "            zip(actor_grad, actor_model.trainable_variables)\n",
    "        )\n",
    "\n",
    "    # We compute the loss and update parameters\n",
    "    def learn(self):\n",
    "        # Get sampling range\n",
    "        record_range = min(self.buffer_counter, self.buffer_capacity)\n",
    "        # Randomly sample indices\n",
    "        batch_indices = np.random.choice(record_range, self.batch_size)\n",
    "\n",
    "        # Convert to tensors\n",
    "        state_batch = keras.ops.convert_to_tensor(self.state_buffer[batch_indices])\n",
    "        action_batch = keras.ops.convert_to_tensor(self.action_buffer[batch_indices])\n",
    "        reward_batch = keras.ops.convert_to_tensor(self.reward_buffer[batch_indices])\n",
    "        reward_batch = keras.ops.cast(reward_batch, dtype=\"float32\")\n",
    "        next_state_batch = keras.ops.convert_to_tensor(\n",
    "            self.next_state_buffer[batch_indices]\n",
    "        )\n",
    "\n",
    "        self.update(state_batch, action_batch, reward_batch, next_state_batch)\n",
    "\n",
    "\n",
    "# This update target parameters slowly\n",
    "# Based on rate `tau`, which is much less than one.\n",
    "def update_target(target, original, tau):\n",
    "    target_weights = target.get_weights()\n",
    "    original_weights = original.get_weights()\n",
    "\n",
    "    for i in range(len(target_weights)):\n",
    "        target_weights[i] = original_weights[i] * tau + target_weights[i] * (1 - tau)\n",
    "\n",
    "    target.set_weights(target_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "Below, we outline the specifications for the Actor and Critic networks. Both are constructed using Dense models with ReLU activation functions.\n",
    "\n",
    "For the Actor network's last layer, it's essential to initialize weights within the range of -0.003 to 0.003. This prevents the model from generating initial output values of 1 or -1, which could lead to gradient saturation issues when using the tanh activation function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "def get_actor():\n",
    "    # Initialize weights between -3e-3 and 3-e3\n",
    "    last_init = keras.initializers.RandomUniform(minval=-0.003, maxval=0.003)\n",
    "\n",
    "    inputs = layers.Input(shape=(num_states,))\n",
    "    out = layers.Dense(256, activation=\"relu\")(inputs)\n",
    "    out = layers.Dense(256, activation=\"relu\")(out)\n",
    "    outputs = layers.Dense(1, activation=\"tanh\", kernel_initializer=last_init)(out)\n",
    "\n",
    "    # Our upper bound is 2.0 for Pendulum.\n",
    "    outputs = outputs * upper_bound\n",
    "    model = keras.Model(inputs, outputs)\n",
    "    return model\n",
    "def get_critic():\n",
    "    # State as input\n",
    "    state_input = layers.Input(shape=(num_states,))\n",
    "    state_out = layers.Dense(16, activation=\"relu\")(state_input)\n",
    "    state_out = layers.Dense(32, activation=\"relu\")(state_out)\n",
    "\n",
    "    # Action as input\n",
    "    action_input = layers.Input(shape=(num_actions,))\n",
    "    action_out = layers.Dense(32, activation=\"relu\")(action_input)\n",
    "\n",
    "    # Both are passed through seperate layer before concatenating\n",
    "    concat = layers.Concatenate()([state_out, action_out])\n",
    "\n",
    "    out = layers.Dense(256, activation=\"relu\")(concat)\n",
    "    out = layers.Dense(256, activation=\"relu\")(out)\n",
    "    outputs = layers.Dense(1)(out)\n",
    "\n",
    "    # Outputs single value for give state-action\n",
    "    model = keras.Model([state_input, action_input], outputs)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "`policy()` returns an action sampled from our Actor network plus some noise for\n",
    "exploration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "def policy(state, noise_object):\n",
    "    sampled_actions = keras.ops.squeeze(actor_model(state))\n",
    "    noise = noise_object()\n",
    "    # Adding noise to action\n",
    "    sampled_actions = sampled_actions.numpy() + noise\n",
    "\n",
    "    # We make sure action is within bounds\n",
    "    legal_action = np.clip(sampled_actions, lower_bound, upper_bound)\n",
    "\n",
    "    return [np.squeeze(legal_action)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## Training hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-27 12:52:08.281095: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:998] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-03-27 12:52:08.282213: W tensorflow/core/common_runtime/gpu/gpu_device.cc:2251] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n"
     ]
    }
   ],
   "source": [
    "std_dev = 0.2\n",
    "ou_noise = OUActionNoise(mean=np.zeros(1), std_deviation=float(std_dev) * np.ones(1))\n",
    "\n",
    "actor_model = get_actor()\n",
    "critic_model = get_critic()\n",
    "\n",
    "target_actor = get_actor()\n",
    "target_critic = get_critic()\n",
    "\n",
    "# Making the weights equal initially\n",
    "target_actor.set_weights(actor_model.get_weights())\n",
    "target_critic.set_weights(critic_model.get_weights())\n",
    "\n",
    "# Learning rate for actor-critic models\n",
    "critic_lr = 0.002\n",
    "actor_lr = 0.001\n",
    "\n",
    "critic_optimizer = keras.optimizers.Adam(critic_lr)\n",
    "actor_optimizer = keras.optimizers.Adam(actor_lr)\n",
    "\n",
    "total_episodes = 100\n",
    "# Discount factor for future rewards\n",
    "gamma = 0.99\n",
    "# Used to update target networks\n",
    "tau = 0.005\n",
    "\n",
    "buffer = Buffer(50000, 64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "Now we implement our main training loop, and iterate over episodes.\n",
    "We sample actions using `policy()` and train with `learn()` at each time step,\n",
    "along with updating the Target networks at a rate `tau`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode * 0 * Avg Reward is ==> -1337.7846455286651\n",
      "Episode * 1 * Avg Reward is ==> -1312.727429997055\n",
      "Episode * 2 * Avg Reward is ==> -1339.7754287530145\n",
      "Episode * 3 * Avg Reward is ==> -1366.393257478266\n",
      "Episode * 4 * Avg Reward is ==> -1378.3500175546662\n",
      "Episode * 5 * Avg Reward is ==> -1402.8971504942253\n",
      "Episode * 6 * Avg Reward is ==> -1409.578017251589\n",
      "Episode * 7 * Avg Reward is ==> -1415.2917951002198\n",
      "Episode * 8 * Avg Reward is ==> -1408.411755650625\n",
      "Episode * 9 * Avg Reward is ==> -1417.2895560451482\n",
      "Episode * 10 * Avg Reward is ==> -1424.544566797937\n",
      "Episode * 11 * Avg Reward is ==> -1394.933204265246\n",
      "Episode * 12 * Avg Reward is ==> -1355.7540200424467\n",
      "Episode * 13 * Avg Reward is ==> -1335.1753868573178\n",
      "Episode * 14 * Avg Reward is ==> -1306.9529569314716\n",
      "Episode * 15 * Avg Reward is ==> -1264.733030089569\n",
      "Episode * 16 * Avg Reward is ==> -1211.5557821656191\n",
      "Episode * 17 * Avg Reward is ==> -1151.5902477976263\n",
      "Episode * 18 * Avg Reward is ==> -1156.5520092778856\n",
      "Episode * 19 * Avg Reward is ==> -1105.1084011356372\n",
      "Episode * 20 * Avg Reward is ==> -1064.1508925383357\n",
      "Episode * 21 * Avg Reward is ==> -1058.2767874990905\n",
      "Episode * 22 * Avg Reward is ==> -1017.9488404352434\n",
      "Episode * 23 * Avg Reward is ==> -980.9603422365584\n",
      "Episode * 24 * Avg Reward is ==> -946.7825452219907\n",
      "Episode * 25 * Avg Reward is ==> -910.5132493245751\n",
      "Episode * 26 * Avg Reward is ==> -881.6680361465893\n",
      "Episode * 27 * Avg Reward is ==> -854.7014039190136\n",
      "Episode * 28 * Avg Reward is ==> -829.7554844388125\n",
      "Episode * 29 * Avg Reward is ==> -806.4321339090004\n",
      "Episode * 30 * Avg Reward is ==> -784.2930737567228\n",
      "Episode * 31 * Avg Reward is ==> -763.4718609595973\n",
      "Episode * 32 * Avg Reward is ==> -747.6088709234713\n",
      "Episode * 33 * Avg Reward is ==> -732.915862388345\n",
      "Episode * 34 * Avg Reward is ==> -718.8752480216414\n",
      "Episode * 35 * Avg Reward is ==> -709.2482472746424\n",
      "Episode * 36 * Avg Reward is ==> -693.3496120970216\n",
      "Episode * 37 * Avg Reward is ==> -678.3169363831818\n",
      "Episode * 38 * Avg Reward is ==> -667.1517050412122\n"
     ]
    }
   ],
   "source": [
    "# To store reward history of each episode\n",
    "ep_reward_list = []\n",
    "# To store average reward history of last few episodes\n",
    "avg_reward_list = []\n",
    "\n",
    "# Takes about 4 min to train\n",
    "for ep in range(total_episodes):\n",
    "    prev_state, _ = env.reset()\n",
    "    episodic_reward = 0\n",
    "\n",
    "    while True:\n",
    "        tf_prev_state = keras.ops.expand_dims(\n",
    "            keras.ops.convert_to_tensor(prev_state), 0\n",
    "        )\n",
    "\n",
    "        action = policy(tf_prev_state, ou_noise)\n",
    "        # Recieve state and reward from environment.\n",
    "        state, reward, done, truncated, _ = env.step(action)\n",
    "\n",
    "        buffer.record((prev_state, action, reward, state))\n",
    "        episodic_reward += reward\n",
    "\n",
    "        buffer.learn()\n",
    "\n",
    "        update_target(target_actor, actor_model, tau)\n",
    "        update_target(target_critic, critic_model, tau)\n",
    "\n",
    "        # End this episode when `done` or `truncated` is True\n",
    "        if done or truncated:\n",
    "            break\n",
    "\n",
    "        prev_state = state\n",
    "\n",
    "    ep_reward_list.append(episodic_reward)\n",
    "\n",
    "    # Mean of last 40 episodes\n",
    "    avg_reward = np.mean(ep_reward_list[-40:])\n",
    "    print(\"Episode * {} * Avg Reward is ==> {}\".format(ep, avg_reward))\n",
    "    avg_reward_list.append(avg_reward)\n",
    "\n",
    "# Plotting graph\n",
    "# Episodes versus Avg. Rewards\n",
    "plt.plot(avg_reward_list)\n",
    "plt.xlabel(\"Episode\")\n",
    "plt.ylabel(\"Avg. Episodic Reward\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "As training progresses, the average episodic reward should gradually increase. Experimentation with different learning rates, tau values, and network architectures for both the Actor and Critic networks is encouraged.\n",
    "\n",
    "While the Inverted Pendulum problem may have low complexity, DDPG (Deep Deterministic Policy Gradient) can excel on various other problems as well.\n",
    "\n",
    "Another environment worth exploring with DDPG is `LunarLander-v2` in its continuous version. Keep in mind that achieving good results may require training over more episodes due to the increased complexity of the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "# Save the weights\n",
    "actor_model.save_weights(\"pendulum_actor.weights.h5\")\n",
    "critic_model.save_weights(\"pendulum_critic.weights.h5\")\n",
    "\n",
    "target_actor.save_weights(\"pendulum_target_actor.weights.h5\")\n",
    "target_critic.save_weights(\"pendulum_target_critic.weights.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "Before Training:\n",
    "\n",
    "![before_img](https://i.imgur.com/ox6b9rC.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "After 100 episodes:\n",
    "\n",
    "![after_img](https://i.imgur.com/eEH8Cz6.gif)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "None",
  "colab": {
   "collapsed_sections": [],
   "name": "ddpg_pendulum",
   "private_outputs": false,
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
